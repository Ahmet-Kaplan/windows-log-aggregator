# This is the root config file, which only includes components of the actual configuration

# Do not collect fluentd's own logs to avoid infinite loops.
<label @FLUENT_LOG>
  <match fluent.*>
    @type stdout
  </match>
</label>

<source>
  @type http
  port 9880
  bind 0.0.0.0
  body_size_limit 32m
  keepalive_timeout 10s
</source>

<match *.**>
  @type copy
    
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix fluentd
    logstash_dateformat %Y%m%d
    include_tag_key true
    type_name access_log
    tag_key @log_name
    flush_interval 1s
    user "#{ ENV['ELASTIC_USER'] || 'elastic' }"
    password "#{ ENV['ELASTIC_PASSWORD'] || 'elastic' }"
  </store>
    
  <store>
    @type stdout
  </store>
</match

<match *>
  @type sql
  host  "#{ ENV['SQLSERVER_HOST'] || '0.0.0.0' }"
  port "#{ ENV['SQLSERVER_PORT'] || '1433' }"
  database fluentd
  adapter sqlserver
  username "#{ ENV['SQLSERVER_USER'] || 'sa' }"
  password "#{ ENV['SQLSERVER_PASSWORD'] || 's3cr3tPassW0rd' }"
  
  <table>
    table dbo.logs
    column_mapping 'timestamp:created_at,fluentdata1:dbcol1,fluentdata2:dbcol2,fluentdata3:dbcol3'
    # This is the default table because it has no "pattern" argument in <table>
    # The logic is such that if all non-default <table> blocks
    # do not match, the default one is chosen.
    # The default table is required.
  </table>

</match>


# Confiugre log target for sending raw logs here
# S3: https://docs.fluentd.org/output/s3
<match logs>
  @type                 s3
  @id                   s3_log_target

  aws_key_id            "#{ ENV['AWS_KEY_ID'] || 'DEFAULT_AWS_KEY_ID' }"
  aws_sec_key           "#{ ENV['AWS_SECRET_KEY'] || 'DEFAULT_AWS_SECRET_KEY' }"
  s3_bucket             "#{ ENV['S3_BUCKET_NAME'] || 'DEFAULT_S3_BUCKET_NAME' }"
  s3_region             "#{ ENV['S3_REGION'] || 'DEFAULT_S3_REGION' }"
  path                  logs/
</match>

@include /etc/fluent/config.d/*.conf
